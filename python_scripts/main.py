#!/usr/bin/env python3
"""
Main CLI entry point for web content extraction system.
"""

import sys
from web_content_system import WebContentExtractor


def print_menu():
    """Print main menu."""
    print("\n" + "=" * 50)
    print("ğŸ“‹ è¯·é€‰æ‹©æ“ä½œ:")
    print("1. ğŸ•¸ï¸  æŠ“å–ç½‘é¡µå†…å®¹")
    print("2. âœï¸  æ‰‹å·¥å½•å…¥å†…å®¹")
    print("3. ğŸ“– æŸ¥çœ‹æ•°æ®åº“å†…å®¹")
    print("4. â“ å¸®åŠ©ä¿¡æ¯")
    print("5. ğŸšª é€€å‡ºç³»ç»Ÿ")
    print("6. ğŸš€ æ‰¹é‡æŠ“å– (grab_params.json)")

def process_batch_scraping(extractor):
    """Process batch scraping from grab_params.json."""
    import json
    import os
    
    file_path = "grab_params.json"
    
    if not os.path.exists(file_path):
        print(f"âŒ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {file_path}")
        print("è¯·åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»º grab_params.jsonï¼Œæ ¼å¼å¦‚ä¸‹:")
        print('[{"url": "...", "tags": "...", "done": false}]')
        return

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            items = json.load(f)
        
        if not isinstance(items, list):
            print("âŒ JSONæ ¼å¼é”™è¯¯: æ ¹èŠ‚ç‚¹å¿…é¡»æ˜¯åˆ—è¡¨")
            return
            
        count = 0
        total = len(items)
        print(f"\nğŸ“¦ å¼€å§‹æ‰¹é‡å¤„ç†ï¼Œå…± {total} ä¸ªä»»åŠ¡")
        
        for i, item in enumerate(items):
            if item.get('done', False):
                continue
                
            url = item.get('url')
            if not url:
                print(f"âš ï¸  è·³è¿‡æ— æ•ˆä»»åŠ¡ (ç¼ºå°‘URL): ä»»åŠ¡ #{i+1}")
                continue
                
            print(f"\nğŸ”„ å¤„ç†ä»»åŠ¡ {i+1}/{total}...")
            tags = item.get('tags', '')
            
            try:
                success = extractor.scrape_and_process(url, tags)
                if success:
                    item['done'] = True
                    count += 1
                    
                    # Immediate save to prevent data loss
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(items, f, indent=2, ensure_ascii=False)
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {e}")
                
        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ! æˆåŠŸå¤„ç† {count} ä¸ªæ–°ä»»åŠ¡")
            
    except json.JSONDecodeError:
        print("âŒ JSONæ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¯­æ³•")
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å‡ºé”™: {e}")


def print_help():
    """Print help information."""
    print("\nğŸ“– ä½¿ç”¨å¸®åŠ©:")
    print("â€¢ æŠ“å–ç½‘é¡µå†…å®¹: è¾“å…¥ç½‘é¡µURLï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æŠ“å–å†…å®¹ã€ç”Ÿæˆæ‘˜è¦å¹¶ä¿å­˜")
    print("â€¢ æ‰‹å·¥å½•å…¥å†…å®¹: æ‰‹åŠ¨è¾“å…¥æ ‡é¢˜å’Œå†…å®¹ï¼Œç³»ç»Ÿç”Ÿæˆæ‘˜è¦å¹¶ä¿å­˜")
    print("â€¢ æŸ¥çœ‹æ•°æ®åº“å†…å®¹: æ˜¾ç¤ºæœ€è¿‘æŠ“å–çš„è®°å½•")
    print("\nğŸ”§ APIé…ç½®:")
    print("â€¢ è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æ¥ä½¿ç”¨OpenAI API")
    print("â€¢ è®¾ç½®ç¯å¢ƒå˜é‡ OLLAMA_API_URL æ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
    print("â€¢ å¦‚æœæœªé…ç½®APIï¼Œç³»ç»Ÿå°†ä½¿ç”¨ç®€å•æ‘˜è¦ç®—æ³•")


def main():
    """Main CLI loop."""
    print("ğŸš€ ç½‘é¡µå†…å®¹æŠ“å–ä¸æ‘˜è¦ç”Ÿæˆç³»ç»Ÿ v2.0")
    print("åˆå§‹åŒ–ä¸­...")
    
    try:
        extractor = WebContentExtractor()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–åŒ…: pip install -r requirements.txt")
        return 1
    
    print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    try:
        while True:
            print_menu()
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
            
            if choice == "1":
                url = input("\nğŸ”— è¯·è¾“å…¥ç½‘é¡µURL: ").strip()
                if url:
                    tags = input("ğŸ·ï¸  è¯·è¾“å…¥æ ‡ç­¾ (å¯é€‰ï¼Œå¤šä¸ªæ ‡ç­¾ç”¨é€—å·åˆ†éš”): ").strip()
                    extractor.scrape_and_process(url, tags)
                else:
                    print("âŒ URLä¸èƒ½ä¸ºç©º")
            
            elif choice == "2":
                title = input("\nğŸ“ è¯·è¾“å…¥æ ‡é¢˜: ").strip()
                content = input("ğŸ“„ è¯·è¾“å…¥å†…å®¹: ").strip()
                if title and content:
                    tags = input("ğŸ·ï¸  è¯·è¾“å…¥æ ‡ç­¾ (å¯é€‰ï¼Œå¤šä¸ªæ ‡ç­¾ç”¨é€—å·åˆ†éš”): ").strip()
                    summary = extractor.manual_input(title, content, tags)
                    print(f"\nğŸ“‹ ç”Ÿæˆçš„æ‘˜è¦: {summary}")
                else:
                    print("âŒ æ ‡é¢˜å’Œå†…å®¹ä¸èƒ½ä¸ºç©º")
            
            elif choice == "3":
                try:
                    limit = input("\nğŸ“Š æŸ¥çœ‹æœ€è¿‘å‡ æ¡è®°å½•? (é»˜è®¤10æ¡): ").strip()
                    limit = int(limit) if limit else 10
                except ValueError:
                    limit = 10
                extractor.view_recent_records(limit)
            
            elif choice == "4":
                print_help()
            
            elif choice == "5":
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            elif choice == "6":
                process_batch_scraping(extractor)
            
            else:
                print("\nâŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        return 1
    
    finally:
        extractor.close()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
