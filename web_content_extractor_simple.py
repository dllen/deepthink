"""
ç½‘é¡µå†…å®¹æŠ“å–ä¸æ‘˜è¦ç”Ÿæˆç³»ç»Ÿï¼ˆç®€åŒ–ç‰ˆï¼‰
åŠŸèƒ½ï¼š
1. æŠ“å–ç½‘é¡µé“¾æ¥å†…å®¹
2. è°ƒç”¨å¤§æ¨¡å‹APIç”Ÿæˆæ‘˜è¦
3. ç”Ÿæˆå¾®åšå†…å®¹
4. å­˜å‚¨åˆ°SQLiteæ•°æ®åº“
5. æ”¯æŒæ‰‹å·¥å½•å…¥
6. ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿç”¨æˆ·è®¿é—®
"""

import sqlite3
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import os
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from openai import OpenAI
import sys


class WebContentExtractor:
    def __init__(self):
        self.setup_database()
        self.setup_browser()
        
    def setup_database(self):
        """åˆ›å»ºSQLiteæ•°æ®åº“å’Œè¡¨"""
        self.conn = sqlite3.connect('web_content.db')
        cursor = self.conn.cursor()
        
        # åˆ›å»ºå†…å®¹è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS content_summary (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                created_time TEXT NOT NULL,
                summary TEXT NOT NULL,
                original_url TEXT NOT NULL,
                tags TEXT
            )
        ''')
        
        # ä¸ºtagså­—æ®µåˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_tags ON content_summary (tags)')
        
        # åˆ›å»ºæ‰‹å·¥å½•å…¥è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS manual_content (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                created_time TEXT NOT NULL,
                summary TEXT,
                tags TEXT
            )
        ''')
        
        self.conn.commit()
    
    def setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨é©±åŠ¨"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # åå°è¿è¡Œ
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        except Exception as e:
            print(f"æ— æ³•å¯åŠ¨Chromeæµè§ˆå™¨: {e}")
            print("è¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨å’ŒChromeDriver")
            # å¦‚æœChromeä¸å¯ç”¨ï¼Œä½¿ç”¨requestsä½œä¸ºå¤‡é€‰
            self.driver = None
    
    def extract_content_with_browser(self, url):
        """ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿç”¨æˆ·è®¿é—®æŠ“å–å†…å®¹"""
        if self.driver is None:
            return self.extract_content_with_requests(url)
        
        try:
            self.driver.get(url)
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # è·å–é¡µé¢æ ‡é¢˜
            title = self.driver.title
            
            # å°è¯•è·å–æ›´ç²¾ç¡®çš„å†…å®¹ï¼ˆæ–‡ç« ç±»ï¼‰
            content = ""
            
            # ä¼˜å…ˆæŸ¥æ‰¾æ–‡ç« ç›¸å…³æ ‡ç­¾
            article_elements = self.driver.find_elements(By.TAG_NAME, "article")
            if article_elements:
                content = " ".join([elem.text for elem in article_elements])
            else:
                # å°è¯•è·å–ä¸»è¦å†…å®¹åŒºåŸŸ
                main_elements = self.driver.find_elements(By.TAG_NAME, "main")
                if main_elements:
                    content = main_elements[0].text
                else:
                    # å°è¯•è·å–æœ‰ç‰¹å®šç±»åçš„div
                    content_divs = self.driver.find_elements(By.CSS_SELECTOR, 
                        "div[class*='content'], div[class*='article'], div[class*='post'], div[class*='main']")
                    if content_divs:
                        content = " ".join([elem.text for elem in content_divs])
                    else:
                        # è·å–æ‰€æœ‰æ®µè½
                        p_elements = self.driver.find_elements(By.TAG_NAME, "p")
                        content = " ".join([p.text for p in p_elements if len(p.text) > 20])
            
            # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œå°è¯•è·å–bodyå†…å®¹
            if len(content) < 100:
                body_element = self.driver.find_element(By.TAG_NAME, "body")
                content = body_element.text
            
            return title.strip(), content.strip()
            
        except Exception as e:
            print(f"æµè§ˆå™¨æŠ“å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨requests: {e}")
            return self.extract_content_with_requests(url)
    
    def extract_content_with_requests(self, url):
        """ä½¿ç”¨requestsåº“æŠ“å–å†…å®¹ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
                script.decompose()
            
            title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
            
            # å°è¯•è·å–æ–‡ç« å†…å®¹
            article_content = soup.find('article')
            if article_content:
                content = article_content.get_text()
            else:
                # å°è¯•è·å–ä¸»è¦æ–‡ç« åŒºåŸŸ
                main_content = (soup.find('main') or 
                               soup.find('div', class_=re.compile(r'article|content|main|post|entry')) or
                               soup.find('section', class_=re.compile(r'article|content|main|post|entry')))
                if main_content:
                    content = main_content.get_text()
                else:
                    # è·å–æ‰€æœ‰æ®µè½
                    paragraphs = soup.find_all('p')
                    content = " ".join([p.get_text() for p in paragraphs if len(p.get_text()) > 20])
            
            # æ¸…ç†å†…å®¹
            content = re.sub(r'\s+', ' ', content).strip()
            
            return title.strip(), content
            
        except Exception as e:
            print(f"ä½¿ç”¨requestsæŠ“å–å¤±è´¥: {e}")
            return "æŠ“å–å¤±è´¥", "æ— æ³•è·å–é¡µé¢å†…å®¹"
    
    def generate_summary_with_api(self, content, title):
        """ä½¿ç”¨å¤§æ¨¡å‹APIç”Ÿæˆæ‘˜è¦"""
        # å°è¯•å¤šç§API
        summary = self.try_openai_api(content, title)
        if summary and summary != "æ‘˜è¦ç”Ÿæˆå¤±è´¥":
            return summary
        
        summary = self.try_local_model_api(content, title)
        if summary and summary != "æ‘˜è¦ç”Ÿæˆå¤±è´¥":
            return summary
        
        # å¦‚æœAPIéƒ½å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ‘˜è¦ç®—æ³•
        return self.generate_simple_summary(content, title)
    
    def try_openai_api(self, content, title):
        """å°è¯•ä½¿ç”¨OpenAI API"""
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                return "æ‘˜è¦ç”Ÿæˆå¤±è´¥"
                
            client = OpenAI(
                api_key=api_key
            )
            
            prompt = f"""
            è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€æ´å‡†ç¡®çš„æ‘˜è¦ï¼Œå­—æ•°åœ¨100-200å­—ä¹‹é—´ï¼š

            æ–‡ç« æ ‡é¢˜: {title}

            æ–‡ç« å†…å®¹: {content[:3000]}

            è¯·ç”Ÿæˆæ‘˜è¦:
            """
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.5
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            return "æ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    def try_local_model_api(self, content, title):
        """å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹APIï¼ˆå¦‚Ollamaï¼‰"""
        try:
            # å°è¯•Ollama API
            ollama_url = os.getenv("OLLAMA_API_URL", "http://localhost:11434/api/generate")
            import json
            
            prompt = f"""
            è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€æ´å‡†ç¡®çš„æ‘˜è¦ï¼Œå­—æ•°åœ¨100-200å­—ä¹‹é—´ï¼š

            æ–‡ç« æ ‡é¢˜: {title}

            æ–‡ç« å†…å®¹: {content[:3000]}

            è¯·ç”Ÿæˆæ‘˜è¦:
            """
            
            data = {
                "model": os.getenv("OLLAMA_MODEL", "llama2"),
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(ollama_url, json=data, timeout=30)
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "æ‘˜è¦ç”Ÿæˆå¤±è´¥").strip()
            
        except Exception as e:
            print(f"æœ¬åœ°æ¨¡å‹APIè°ƒç”¨å¤±è´¥: {e}")
        
        return "æ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    def generate_simple_summary(self, content, title):
        """ä½¿ç”¨ç®€å•ç®—æ³•ç”Ÿæˆæ‘˜è¦ï¼ˆå½“APIä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        try:
            # ç®€å•æ‘˜è¦ç®—æ³•ï¼šæå–å‰å‡ æ®µè½çš„å…³é”®å¥å­
            paragraphs = [p.strip() for p in content.split('\n') if len(p.strip()) > 20]
            
            if not paragraphs:
                return content[:200] + "..." if len(content) > 200 else content
            
            # å–å‰å‡ ä¸ªæ®µè½
            summary_parts = []
            total_chars = 0
            
            for para in paragraphs:
                if total_chars >= 100:  # è‡³å°‘100å­—
                    break
                summary_parts.append(para)
                total_chars += len(para)
            
            summary = " ".join(summary_parts)
            if len(summary) > 200:
                summary = summary[:200] + "..."
            
            return summary
        except Exception as e:
            print(f"ç®€å•æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return content[:200] + "..." if len(content) > 200 else content
    
    def generate_weibo_content(self, title, summary, url):
        """ç”Ÿæˆå¾®åšå†…å®¹"""
        # å¾®åšæ ‡é¢˜é™åˆ¶
        title_part = title[:20] if len(title) > 20 else title
        # æ‘˜è¦éƒ¨åˆ†
        summary_part = summary[:80] if len(summary) > 80 else summary
        
        weibo_content = f"ã€{title_part}ã€‘{summary_part} æ›´å¤šå†…å®¹: {url}"
        
        # ç¡®ä¿æ€»é•¿åº¦é€‚åˆå¾®åš
        if len(weibo_content) > 140:
            weibo_content = f"ã€{title_part}ã€‘{summary_part[:60]}... è¯¦æƒ…: {url}"
        
        return weibo_content
    
    def save_to_database(self, title, summary, url, tags=""):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        cursor = self.conn.cursor()
        created_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        cursor.execute('''
            INSERT INTO content_summary (title, created_time, summary, original_url, tags)
            VALUES (?, ?, ?, ?, ?)
        ''', (title, created_time, summary, url, tags))
        
        self.conn.commit()
        print(f"âœ“ å†…å®¹å·²ä¿å­˜åˆ°æ•°æ®åº“: {title[:50]}...")
    
    def manual_input(self, title, content, tags=""):
        """æ‰‹å·¥å½•å…¥å†…å®¹"""
        print("æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
        summary = self.generate_summary_with_api(content, title)
        
        cursor = self.conn.cursor()
        created_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        cursor.execute('''
            INSERT INTO manual_content (title, content, created_time, summary, tags)
            VALUES (?, ?, ?, ?, ?)
        ''', (title, content, created_time, summary, tags))
        
        self.conn.commit()
        print(f"âœ“ æ‰‹å·¥å½•å…¥å†…å®¹å·²ä¿å­˜: {title[:50]}...")
        return summary
    
    def scrape_and_process(self, url, tags=""):
        """æŠ“å–å¹¶å¤„ç†ç½‘é¡µ"""
        print(f"ğŸ” å¼€å§‹æŠ“å–: {url}")
        
        title, content = self.extract_content_with_browser(url)
        
        if not content or len(content.strip()) == 0 or "æŠ“å–å¤±è´¥" in title:
            print("âŒ æ— æ³•æŠ“å–åˆ°æœ‰æ•ˆå†…å®¹")
            return False
        
        print(f"âœ… æŠ“å–æˆåŠŸ - æ ‡é¢˜: {title[:50]}{'...' if len(title) > 50 else ''}")
        print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # ç”Ÿæˆæ‘˜è¦
        print("â³ æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
        summary = self.generate_summary_with_api(content, title)
        print(f"ğŸ“‹ æ‘˜è¦: {summary}")
        
        # ç”Ÿæˆå¾®åšå†…å®¹
        weibo_content = self.generate_weibo_content(title, summary, url)
        print(f">Weibo: {weibo_content}")
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self.save_to_database(title, summary, url, tags)
        
        return True
    
    def view_recent_records(self, limit=10):
        """æŸ¥çœ‹æœ€è¿‘çš„è®°å½•"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT * FROM content_summary ORDER BY id DESC LIMIT ?", (limit,))
        results = cursor.fetchall()
        
        if not results:
            print("æš‚æ— æŠ“å–è®°å½•")
            return
        
        print(f"\nğŸ“Š æœ€è¿‘{len(results)}æ¡æŠ“å–è®°å½•:")
        print("="*80)
        for row in results:
            print(f"ID: {row[0]}")
            print(f"æ ‡é¢˜: {row[1][:50]}{'...' if len(row[1]) > 50 else ''}")
            print(f"æ—¶é—´: {row[2]}")
            print(f"æ‘˜è¦: {row[3][:100]}{'...' if len(row[3]) > 100 else ''}")
            print(f"URL: {row[4][:50]}{'...' if len(row[4]) > 50 else ''}")
            print(f"æ ‡ç­¾: {row[5] if row[5] else 'æ— '}")
            print("-" * 80)
    
    def close(self):
        """å…³é—­èµ„æº"""
        if hasattr(self, 'conn'):
            self.conn.close()
        if hasattr(self, 'driver') and self.driver:
            self.driver.quit()


def main():
    print("ğŸš€ ç½‘é¡µå†…å®¹æŠ“å–ä¸æ‘˜è¦ç”Ÿæˆç³»ç»Ÿ")
    print("åˆå§‹åŒ–ä¸­...")
    
    try:
        extractor = WebContentExtractor()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–åŒ…: pip install -r requirements.txt")
        return
    
    print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    try:
        while True:
            print("\n" + "="*50)
            print("ğŸ“‹ è¯·é€‰æ‹©æ“ä½œ:")
            print("1. ğŸ•¸ï¸  æŠ“å–ç½‘é¡µå†…å®¹")
            print("2. âœï¸  æ‰‹å·¥å½•å…¥å†…å®¹")
            print("3. ğŸ“– æŸ¥çœ‹æ•°æ®åº“å†…å®¹")
            print("4. â“ å¸®åŠ©ä¿¡æ¯")
            print("5. ğŸšª é€€å‡ºç³»ç»Ÿ")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
            
            if choice == "1":
                url = input("\nğŸ”— è¯·è¾“å…¥ç½‘é¡µURL: ").strip()
                if url:
                    tags = input("ğŸ·ï¸  è¯·è¾“å…¥æ ‡ç­¾ (å¯é€‰ï¼Œå¤šä¸ªæ ‡ç­¾ç”¨é€—å·åˆ†éš”): ").strip()
                    extractor.scrape_and_process(url, tags)
                else:
                    print("âŒ URLä¸èƒ½ä¸ºç©º")
            
            elif choice == "2":
                title = input("\nğŸ“ è¯·è¾“å…¥æ ‡é¢˜: ").strip()
                content = input("ğŸ“„ è¯·è¾“å…¥å†…å®¹: ").strip()
                if title and content:
                    tags = input("ğŸ·ï¸  è¯·è¾“å…¥æ ‡ç­¾ (å¯é€‰ï¼Œå¤šä¸ªæ ‡ç­¾ç”¨é€—å·åˆ†éš”): ").strip()
                    summary = extractor.manual_input(title, content, tags)
                    print(f"\nğŸ“‹ ç”Ÿæˆçš„æ‘˜è¦: {summary}")
                else:
                    print("âŒ æ ‡é¢˜å’Œå†…å®¹ä¸èƒ½ä¸ºç©º")
            
            elif choice == "3":
                try:
                    limit = input("\nğŸ“Š æŸ¥çœ‹æœ€è¿‘å‡ æ¡è®°å½•? (é»˜è®¤10æ¡): ").strip()
                    limit = int(limit) if limit else 10
                except ValueError:
                    limit = 10
                extractor.view_recent_records(limit)
            
            elif choice == "4":
                print("\nğŸ“– ä½¿ç”¨å¸®åŠ©:")
                print("â€¢ æŠ“å–ç½‘é¡µå†…å®¹: è¾“å…¥ç½‘é¡µURLï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æŠ“å–å†…å®¹ã€ç”Ÿæˆæ‘˜è¦å¹¶ä¿å­˜")
                print("â€¢ æ‰‹å·¥å½•å…¥å†…å®¹: æ‰‹åŠ¨è¾“å…¥æ ‡é¢˜å’Œå†…å®¹ï¼Œç³»ç»Ÿç”Ÿæˆæ‘˜è¦å¹¶ä¿å­˜")
                print("â€¢ æŸ¥çœ‹æ•°æ®åº“å†…å®¹: æ˜¾ç¤ºæœ€è¿‘æŠ“å–çš„è®°å½•")
                print("\nğŸ”§ APIé…ç½®:")
                print("â€¢ è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æ¥ä½¿ç”¨OpenAI API")
                print("â€¢ è®¾ç½®ç¯å¢ƒå˜é‡ OLLAMA_API_URL æ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
            
            elif choice == "5":
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            else:
                print("\nâŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
    
    finally:
        extractor.close()


if __name__ == "__main__":
    main()